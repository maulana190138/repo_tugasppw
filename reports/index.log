Traceback (most recent call last):
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\jupyter_cache\executors\utils.py", line 51, in single_nb_execution
    executenb(
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 646, in run_until_complete
    return future.result()
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python310\lib\site-packages\nbclient\client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        start_urls = [
            'https://pta.trunojoyo.ac.id/welcome/index/10',
            'https://pta.trunojoyo.ac.id/welcome/index/11',
            'https://pta.trunojoyo.ac.id/welcome/index/12',
            'https://pta.trunojoyo.ac.id/welcome/index/13',
            'https://pta.trunojoyo.ac.id/welcome/index/14',
        ]

        for url in start_urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for item in response.css('#content_journal > ul > li'):
            yield {
                'Link': item.css(f'div:nth-child(3) > div:nth-child(5) > a::attr(href)').get(),
            }
------------------

[1;31m---------------------------------------------------------------------------[0m
[1;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Input [1;32mIn [1][0m, in [0;36m<cell line: 1>[1;34m()[0m
[1;32m----> 1[0m [38;5;28;01mimport[39;00m [38;5;21;01mscrapy[39;00m
[0;32m      4[0m [38;5;28;01mclass[39;00m [38;5;21;01mQuotesSpider[39;00m(scrapy[38;5;241m.[39mSpider):
[0;32m      5[0m     name [38;5;241m=[39m [38;5;124m"[39m[38;5;124mquotes[39m[38;5;124m"[39m

[1;31mModuleNotFoundError[0m: No module named 'scrapy'
ModuleNotFoundError: No module named 'scrapy'

