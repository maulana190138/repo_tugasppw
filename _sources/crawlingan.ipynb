{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f41660",
   "metadata": {},
   "source": [
    "# Ngecrawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fefe30",
   "metadata": {},
   "source": [
    "<h3>Crawling data</h3>\n",
    "\n",
    "<p>code dibawah digunakan untuk mengcrawling data dari sebuah website yang nantinya akan diolah dengan stemming, tf-idf dan lian-lain</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f21e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        start_urls = [\n",
    "            'https://pta.trunojoyo.ac.id/welcome/index/10',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/index/11',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/index/12',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/index/13',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/index/14',\n",
    "        ]\n",
    "\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.css('#content_journal > ul > li'):\n",
    "            yield {\n",
    "                'Link': item.css(f'div:nth-child(3) > div:nth-child(5) > a::attr(href)').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab00d1",
   "metadata": {},
   "source": [
    "<p>code diatas digunakan untuk mengambil link setiap dari website yang nantinya akan diproses lebih lebih lanjut menggunakan kode dibawah, data yang dihasilkan nantinya akan dikonversi ke dalam file csv<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccddfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quote\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        data_csv = pd.read_csv('hasil_crawl.csv').values\n",
    "        start_urls = [ link[0] for link in data_csv ]\n",
    "\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield {\n",
    "            'Judul': response.css('#content_journal > ul > li > div:nth-child(2) > a::text').get(),\n",
    "            'Abstraksi': response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > p::text').get(),\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a263fde",
   "metadata": {},
   "source": [
    "<p>untuk code diatas adalah code lanjutan dari sebelumnya yang digunakan untuk mengambil data dari link yang sudah dicrawl code sebelumnya, filenya dimasukkan kedalam file csv yang nantinya akan diproses untuk kepentingan lainnya seperti tf-idf dan lain-lain</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
